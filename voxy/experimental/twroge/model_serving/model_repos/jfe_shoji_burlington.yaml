models:
# Door State Configs
  - artifact_model_paths:
      - artifacts_02_09_2023_old_generalized_side_door/91f587b1-a464-4bb2-9124-4634a12ee929.pt 
      - artifacts_04_05_2023_jfe_shoji_burlington_0011_cha/80fe5e4f-b980-4b09-a037-b80b95c07ef9.pt
      - artifacts_05_02_2023_jfe_shoji_burlington_0010_cha/552c8b65-55bc-4d91-937c-57686f21f67b.pt
    config:
      platform: pytorch_libtorch
      max_batch_size: 16
      input:
        - name: images
          data_type: TYPE_FP32
          format: FORMAT_NCHW
          dims: [3, 224, 224]
      output:
        - name: output0
          data_type: TYPE_FP32
          dims: [3]
# Hardhat/Safety Vest configuration
  - artifact_model_paths:
      - artifacts_04_18_2023_voxel_safetyvest_vit_general_2022-09-21-jit/voxel_safetyvest_vit_general_2022-09-21-jit.pt
      - artifacts_04_18_2023_voxel_safetyvest_vit_laredo_walton_2022-10-05-jit/voxel_safetyvest_vit_laredo_walton_2022-10-05-jit.pt
      - artifacts_bumpcap_vit_99_46_34k_2022-12-20-jit/bumpcap_vit_99_46_34k_2022-12-20-jit.pt
    config:
      platform: pytorch_libtorch
      max_batch_size: 32
      input:
        - name: images
          data_type: TYPE_FP32
          format: FORMAT_NCHW
          dims: [3, 224, 224]
      output:
        - name: output0
          data_type: TYPE_FP32
          dims: [2]
# Human Keypoint Detection Configs
  - artifact_model_paths:
      - artifacts_03_21_2023_pose_0630_jit_update/fast_res50_256x192.pt
    config:
      platform: pytorch_libtorch
      max_batch_size: 128
      input:
        - name: images
          data_type: TYPE_FP32
          format: FORMAT_NCHW
          dims: [3, 256, 192]
      output:
        - name: output0
          data_type: TYPE_FP32
          dims: [17, 64, 48]
          dims: [2]
# Carry object classifier
  - artifact_model_paths:
     -  artifacts_03_24_2023_carry_classifier_jit/best_lift_DSv4_RN34-jit.pt
    config:
      platform: pytorch_libtorch
      max_batch_size: 32
      input:
        - name: images
          data_type: TYPE_FP32
          format: FORMAT_NCHW
          dims: [3, 224, 224]
      output:
        - name: output0
          data_type: TYPE_FP32
          dims: [2]
# Ergonomic Overreach Configs
  - artifact_model_paths:
      - artifacts_03_23_2023_overreaching_model_jit/voxel_ergo_ml_overreaching_2022-05-23-jit.pt
    config:
      platform: pytorch_libtorch
      max_batch_size: 128
      input:
        - name: images
          data_type: TYPE_FP32
          format: FORMAT_NONE
          dims: [30]
      output:
        - name: output0
          data_type: TYPE_FP32
          dims: [2]
# Object Detection Configs
  - artifact_model_paths:
      - artifacts_yolo_v5_pre_processing_04_13_2023/yolo_torchscript_preprocess_apr_13.pt
    disable_warmup_generation: true
    config:
      platform: pytorch_libtorch
      max_batch_size: 16
      input:
        - name: INPUT_0
          data_type: TYPE_UINT8
          format: FORMAT_NHWC
          dims: [-1, -1, 3]
        - name: INPUT_1
          data_type: TYPE_INT32
          dims: [2]
      output:
        - name: OUTPUT_0
          data_type: TYPE_FP16
          dims: [3, -1, -1]
        - name: OUTPUT_1
          data_type: TYPE_FP32
          dims: [2]
        - name: OUTPUT_2
          data_type: TYPE_FP32
          dims: [2]
  - artifact_model_paths:
      - artifacts_yolo_v5_post_processing_04_11_2023/yolo_torchscript_postprocess_apr_11.pt
    disable_warmup_generation: true
    config:
      platform: pytorch_libtorch
      max_batch_size: 16
      input:
        - name: input0
          data_type: TYPE_FP16
          dims: [-1, -1]
        - name: input1
          data_type: TYPE_FP32
          dims: [2]
        - name: input2
          data_type: TYPE_FP32
          dims: [2]
        - name: input3
          data_type: TYPE_INT32
          dims: [2]
        - name: input4
          data_type: TYPE_FP16
          dims: [1]
        - name: input5
          data_type: TYPE_FP16
          dims: [1]
      output:
        - name: output0
          data_type: TYPE_INT64
          dims: [1]
        - name: output1
          data_type: TYPE_FP32
          dims: [-1]
  - artifact_model_paths:
      - artifacts_02_27_2023_michaels_wesco_office_yolo/best_736_1280.engine
    disable_warmup_generation: true
    config:
      platform: tensorrt_plan
      max_batch_size: 2
      input:
        - name: images
          data_type: TYPE_FP16
          format: FORMAT_NCHW
          dims: [3, -1, -1]
      output:
        - name: output0
          data_type: TYPE_FP16
          dims: [-1, -1]
ensembles:
# Object Detection 2D Ensemble
  - primary_model_name: yolo_model
    artifact_model_paths:
      - artifacts_02_27_2023_michaels_wesco_office_yolo/best_736_1280.engine
    config:
      platform: ensemble
      max_batch_size: 1
      input:
        - name: INPUT_0
          data_type: TYPE_UINT8
          format: FORMAT_NHWC
          dims: [-1, -1, 3]
        - name: INPUT_1
          data_type: TYPE_INT32
          dims: [2]
        - name: INPUT_2
          data_type: TYPE_INT32
          dims: [-1]
        - name: INPUT_3
          data_type: TYPE_FP16
          dims: [1]
        - name: INPUT_4
          data_type: TYPE_FP16
          dims: [1]
      output:
        - name: output0
          data_type: TYPE_INT64
          dims: [1]
        - name: output1
          data_type: TYPE_FP32
          dims: [9]
      ensemble_scheduling:
        step:
          - model_name: artifacts_yolo_v5_pre_processing_04_13_2023/yolo_torchscript_preprocess_apr_13.pt
            model_version: 1
            input_map:
              INPUT_0: INPUT_0
              INPUT_1: INPUT_1
            output_map:
              OUTPUT_0: images
              OUTPUT_1: offset
              OUTPUT_2: scale
          - model_name: yolo_model
            model_version: 1
            input_map:
              images: images
            output_map:
              output0: predictions
          - model_name: artifacts_yolo_v5_post_processing_04_11_2023/yolo_torchscript_postprocess_apr_11.pt
            model_version: 1
            input_map:
              input0: predictions
              input1: offset
              input2: scale
              input3: INPUT_2
              input4: INPUT_3
              input5: INPUT_4
            output_map:
              output0: output0
              output1: output1
