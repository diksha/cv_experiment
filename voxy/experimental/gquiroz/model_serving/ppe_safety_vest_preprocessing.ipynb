{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import json, sys, os\n",
    "from packaging import version\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "LEGACY_FEATURE_EXTRACTOR = ViTFeatureExtractor.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\", return_tensors=\"pt\"\n",
    ")\n",
    "UPDATED_FEATURE_EXTRACTOR = ViTFeatureExtractor.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\", return_tensors=\"pt\"\n",
    ")\n",
    "# torchvision crop will pad output with 0s if input_image_shape < output_image_shape\n",
    "from torchvision.transforms.functional import crop\n",
    "\n",
    "sys.path.append(os.environ[\"BUILD_WORKSPACE_DIRECTORY\"])\n",
    "from core.labeling.label_store.label_reader import LabelReader\n",
    "from core.utils.video_reader import S3VideoReader, S3VideoReaderInput\n",
    "from core.structs.actor import Actor\n",
    "from core.structs.attributes import RectangleXYWH, RectangleXYXY\n",
    "\n",
    "if version.parse(transformers.__version__) >= version.parse(\"4.25.1\"):\n",
    "    from transformers import ViTImageProcessor\n",
    "\n",
    "    UPDATED_FEATURE_EXTRACTOR = ViTImageProcessor.from_pretrained(\n",
    "        \"google/vit-base-patch16-224-in21k\", return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_UUID = \"uscold/quakertown/0001/cha/detector/1660283690_1660283700_0000\"\n",
    "actor_labels = {\n",
    "    output[\"relative_timestamp_ms\"]: [\n",
    "        Actor.from_dict(actor)\n",
    "        for actor in output[\"actors\"]\n",
    "        if actor[\"category\"] == \"PERSON_V2\"\n",
    "    ]\n",
    "    for output in json.loads(LabelReader().read(VIDEO_UUID))[\"frames\"]\n",
    "}\n",
    "video_frames = {\n",
    "    output.relative_timestamp_ms: output.image\n",
    "    for output in S3VideoReader(S3VideoReaderInput(VIDEO_UUID)).read()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c8c557",
   "metadata": {},
   "source": [
    "## Crop Images Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edfb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_legacy(\n",
    "    actor_bboxes: typing.List[Actor],\n",
    "    frame: np.ndarray,\n",
    "    padding: int,\n",
    "):\n",
    "    cropped_images = []\n",
    "    for actor in actors:\n",
    "        rect = RectangleXYWH.from_polygon(actor.polygon)\n",
    "\n",
    "        cropped_image = frame[\n",
    "            max(0, rect.top_left_vertice.y - padding) : min(\n",
    "                frame.shape[0], rect.top_left_vertice.y + rect.h + padding\n",
    "            ),\n",
    "            max(0, rect.top_left_vertice.x - padding) : min(\n",
    "                frame.shape[1], rect.top_left_vertice.x + rect.w + padding\n",
    "            ),\n",
    "        ]\n",
    "        cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
    "        cropped_images.append(cropped_image)\n",
    "    return cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_images_tensor(\n",
    "    actors_xyxy: typing.List[torch.Tensor],\n",
    "    frame: torch.Tensor,\n",
    "    padding: int,\n",
    "):\n",
    "    cropped_images = []\n",
    "    for xyxy in actors_xyxy:\n",
    "        cropped_images = []\n",
    "        for xyxy in actors_xyxy:\n",
    "            cropped_image = frame[\n",
    "                max(0, int(xyxy[1] - padding)) : min(\n",
    "                    int(frame.shape[0]), int(xyxy[3] + padding)\n",
    "                ),\n",
    "                max(0, int(xyxy[0] - padding)) : min(\n",
    "                    int(frame.shape[1]), int(xyxy[2] + padding)\n",
    "                ),\n",
    "                :,\n",
    "            ]\n",
    "            cropped_image_rgb = cropped_image.flip(2)\n",
    "            cropped_images.append(cropped_image_rgb)\n",
    "    return cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5623bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def legacy_preprocess(\n",
    "    actor_bboxes: typing.List[Actor],\n",
    "    frame: np.ndarray,\n",
    "    padding: int,\n",
    "):\n",
    "    image_crops = crop_image_legacy(actor_bboxes, frame, padding)\n",
    "    if len(image_crops) == 0:\n",
    "        return None\n",
    "    return LEGACY_FEATURE_EXTRACTOR(image_crops, return_tensors=\"pt\")[\n",
    "        \"pixel_values\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def torch_preprocess(\n",
    "    actors_xyxy: typing.List[torch.Tensor],\n",
    "    frame: torch.Tensor,\n",
    "    padding: int,\n",
    "):\n",
    "    image_crops = crop_images_tensor(actors_xyxy, frame, padding)\n",
    "    if len(image_crops) == 0:\n",
    "        return None\n",
    "    return UPDATED_FEATURE_EXTRACTOR(image_crops, return_tensors=\"pt\")[\n",
    "        \"pixel_values\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_images_legacy = {}\n",
    "preprocessed_images_torch = {}\n",
    "PADDING = 5\n",
    "for ts in actor_labels.keys():\n",
    "    actors = actor_labels[ts]\n",
    "    frame = video_frames[ts]\n",
    "    preprocessed_images_legacy[ts] = legacy_preprocess(actors, frame, PADDING)\n",
    "    preprocessed_images_torch[ts] = torch_preprocess(\n",
    "        [\n",
    "            torch.tensor(RectangleXYXY.from_polygon(actor.polygon).to_list())\n",
    "            for actor in actors\n",
    "        ],\n",
    "        torch.from_numpy(frame),\n",
    "        PADDING,\n",
    "    )\n",
    "for ts in preprocessed_images_legacy.keys():\n",
    "    legacy_input = preprocessed_images_legacy[ts]\n",
    "    new_input = preprocessed_images_torch[ts]\n",
    "    assert torch.equal(legacy_input, new_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
