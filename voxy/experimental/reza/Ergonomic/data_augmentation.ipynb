{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54c207b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.environ[\"BUILD_WORKSPACE_DIRECTORY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d331023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os.path\n",
    "import random\n",
    "import sys\n",
    "import xml.etree.ElementTree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.data\n",
    "import cv2\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "def load_occluders(pascal_voc_root_path):\n",
    "    occluders = []\n",
    "    structuring_element = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8, 8))\n",
    "    \n",
    "    annotation_paths = list_filepaths(os.path.join(pascal_voc_root_path, 'Annotations'))\n",
    "    for annotation_path in annotation_paths:\n",
    "        xml_root = xml.etree.ElementTree.parse(annotation_path).getroot()\n",
    "        is_segmented = (xml_root.find('segmented').text != '0')\n",
    "\n",
    "        if not is_segmented:\n",
    "            continue\n",
    "\n",
    "        boxes = []\n",
    "        for i_obj, obj in enumerate(xml_root.findall('object')):\n",
    "            is_person = (obj.find('name').text == 'person')\n",
    "            is_difficult = (obj.find('difficult').text != '0')\n",
    "            is_truncated = (obj.find('truncated').text != '0')\n",
    "            if not is_person and not is_difficult and not is_truncated:\n",
    "                bndbox = obj.find('bndbox')\n",
    "                box = [int(bndbox.find(s).text) for s in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "                boxes.append((i_obj, box))\n",
    "\n",
    "        if not boxes:\n",
    "            continue\n",
    "\n",
    "        im_filename = xml_root.find('filename').text\n",
    "        seg_filename = im_filename.replace('jpg', 'png')\n",
    "\n",
    "        im_path = os.path.join(pascal_voc_root_path, 'JPEGImages', im_filename)\n",
    "        seg_path = os.path.join(pascal_voc_root_path,'SegmentationObject', seg_filename)\n",
    "\n",
    "        im = np.asarray(PIL.Image.open(im_path))\n",
    "        labels = np.asarray(PIL.Image.open(seg_path))\n",
    "\n",
    "        for i_obj, (xmin, ymin, xmax, ymax) in boxes:\n",
    "            object_mask = (labels[ymin:ymax, xmin:xmax] == i_obj + 1).astype(np.uint8)*255\n",
    "            object_image = im[ymin:ymax, xmin:xmax]\n",
    "            if cv2.countNonZero(object_mask) < 500:\n",
    "                # Ignore small objects\n",
    "                continue\n",
    "\n",
    "            # Reduce the opacity of the mask along the border for smoother blending\n",
    "            eroded = cv2.erode(object_mask, structuring_element)\n",
    "            object_mask[eroded < object_mask] = 192\n",
    "            object_with_mask = np.concatenate([object_image, object_mask[..., np.newaxis]], axis=-1)\n",
    "            \n",
    "            # Downscale for efficiency\n",
    "            object_with_mask = resize_by_factor(object_with_mask, 0.75)\n",
    "            occluders.append(object_with_mask)\n",
    "\n",
    "    return occluders\n",
    "\n",
    "\n",
    "def occlude_with_objects(im, occluders):\n",
    "    \"\"\"Returns an augmented version of `im`, containing some occluders from the Pascal VOC dataset.\"\"\"\n",
    "\n",
    "    result = im.copy()\n",
    "    width_height = np.asarray([im.shape[1], im.shape[0]])\n",
    "    im_scale_factor = min(width_height) / 256\n",
    "    count = np.random.randint(1, 8)\n",
    "\n",
    "    for _ in range(count):\n",
    "        occluder = random.choice(occluders)\n",
    "\n",
    "        random_scale_factor = np.random.uniform(0.4, 1.0)\n",
    "        scale_factor = random_scale_factor * im_scale_factor\n",
    "        occluder = resize_by_factor(occluder, scale_factor)\n",
    "\n",
    "        center = np.random.uniform([0,0], width_height)\n",
    "        paste_over(im_src=occluder, im_dst=result, center=center)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def paste_over(im_src, im_dst, center):\n",
    "    \"\"\"Pastes `im_src` onto `im_dst` at a specified position, with alpha blending, in place.\n",
    "    Locations outside the bounds of `im_dst` are handled as expected (only a part or none of\n",
    "    `im_src` becomes visible).\n",
    "    Args:\n",
    "        im_src: The RGBA image to be pasted onto `im_dst`. Its size can be arbitrary.\n",
    "        im_dst: The target image.\n",
    "        alpha: A float (0.0-1.0) array of the same size as `im_src` controlling the alpha blending\n",
    "            at each pixel. Large values mean more visibility for `im_src`.\n",
    "        center: coordinates in `im_dst` where the center of `im_src` should be placed.\n",
    "    \"\"\"\n",
    "\n",
    "    width_height_src = np.asarray([im_src.shape[1], im_src.shape[0]])\n",
    "    width_height_dst = np.asarray([im_dst.shape[1], im_dst.shape[0]])\n",
    "\n",
    "    center = np.round(center).astype(np.int32)\n",
    "    raw_start_dst = center - width_height_src // 2\n",
    "    raw_end_dst = raw_start_dst + width_height_src\n",
    "\n",
    "    start_dst = np.clip(raw_start_dst, 0, width_height_dst)\n",
    "    end_dst = np.clip(raw_end_dst, 0, width_height_dst)\n",
    "    region_dst = im_dst[start_dst[1]:end_dst[1], start_dst[0]:end_dst[0]]\n",
    "\n",
    "    start_src = start_dst - raw_start_dst\n",
    "    end_src = width_height_src + (end_dst - raw_end_dst)\n",
    "    region_src = im_src[start_src[1]:end_src[1], start_src[0]:end_src[0]]\n",
    "    color_src = region_src[..., 0:3]\n",
    "    alpha = region_src[..., 3:].astype(np.float32)/255\n",
    "\n",
    "    im_dst[start_dst[1]:end_dst[1], start_dst[0]:end_dst[0]] = (\n",
    "            alpha * color_src + (1 - alpha) * region_dst)\n",
    "\n",
    "\n",
    "def resize_by_factor(im, factor):\n",
    "    \"\"\"Returns a copy of `im` resized by `factor`, using bilinear interp for up and area interp\n",
    "    for downscaling.\n",
    "    \"\"\"\n",
    "    new_size = tuple(np.round(np.array([im.shape[1], im.shape[0]]) * factor).astype(int))\n",
    "    interp = cv2.INTER_LINEAR if factor > 1.0 else cv2.INTER_AREA\n",
    "    return cv2.resize(im, new_size, fx=factor, fy=factor, interpolation=interp)\n",
    "\n",
    "\n",
    "def list_filepaths(dirpath):\n",
    "    names = os.listdir(dirpath)\n",
    "    paths = [os.path.join(dirpath, name) for name in names]\n",
    "    return sorted(filter(os.path.isfile, paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd53cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "occluders = load_occluders('/home/reza_voxelsafety_com/VOCdevkit/VOC2012/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40d533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.random_erasing import RandomErasing\n",
    "random_erase = RandomErasing(probability=1, min_area = 0.1, max_area = 0.2, mode='const', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb7c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.infra.cloud.gcs_cv2_utils import upload_cv2_image_to_gcs\n",
    "from core.infra.cloud.gcs_utils import dump_to_gcs, get_storage_client\n",
    "output_path = \"gs://voxel-users/reza/ergo_syntheticdata_occlusion\"\n",
    "storage_client = get_storage_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69d25d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "transform = transforms.ToPILImage()\n",
    "\n",
    "def get_occlusion(data, dict_sample, max_id, root_dir):\n",
    "    annotations = data['annotations'].copy()\n",
    "    annotations_new = []\n",
    "    annotations_new.extend(annotations)\n",
    "    save_subfolder = root_dir.split('/')[-1]\n",
    "    for img_name, annos in dict_sample.items():\n",
    "        sample_image = Image.open(os.path.join(root_dir, img_name))\n",
    "        img_array = np.asarray(sample_image)[...,:3]\n",
    "        sample_image = Image.fromarray(img_array.astype('uint8'), 'RGB')\n",
    "        upload_cv2_image_to_gcs(\n",
    "                     cv2.cvtColor(np.asarray(sample_image), cv2.COLOR_BGR2RGB),\n",
    "                     f\"{output_path}/{save_subfolder}/{img_name}\",\n",
    "                    storage_client=storage_client,\n",
    "                )\n",
    "        for i in range(2):\n",
    "            max_id = max_id + 1\n",
    "            Image1copy = sample_image.copy()\n",
    "            image_name1 = f\"occlude1_{i}_{img_name}\"\n",
    "            occlude_key1 = {'id': max_id, 'file_name': image_name1, 'width': sample_image.width, 'height': sample_image.height}\n",
    "            Image2copy = sample_image.copy()\n",
    "            data['images'].append(occlude_key1)\n",
    "            image_name2 = f\"occlude2_{i}_{img_name}\"\n",
    "            max_id = max_id + 1\n",
    "            occlude_key2 = {'id': max_id, 'file_name': image_name2, 'width': sample_image.width, 'height': sample_image.height}\n",
    "            data['images'].append(occlude_key2)\n",
    "            for anno in annos:\n",
    "                anno_new = anno.copy()\n",
    "                anno_new['image_id'] = occlude_key1['id']\n",
    "                annotations_new.append(anno)\n",
    "                anno_new['image_id'] = occlude_key2['id']\n",
    "                annotations_new.append(anno)\n",
    "                if anno['category_id'] == 0:\n",
    "                    #print(f\"keypoints: {anno['keypoints']}\")\n",
    "                    box = anno['bbox']\n",
    "                    actor_img = sample_image.crop((int(box[0]), int(box[1]), int(box[0] + box[2]), int(box[1] + box[3])))\n",
    "                    try:\n",
    "                        occluded_image1 = occlude_with_objects(np.asarray(actor_img)[...,:3], occluders)\n",
    "                        occluded_image1_pil = Image.fromarray(occluded_image1.astype('uint8'), 'RGB')\n",
    "                        Image1copy.paste(occluded_image1_pil, (int(box[0]), int(box[1]), int(box[0] + box[2]), int(box[1] + box[3])))\n",
    "                    except:\n",
    "                        pass\n",
    "#                     occluded_image1 = occlude_with_objects(np.asarray(actor_img)[...,:3], occluders)\n",
    "                    occluded_image2 = random_erase(transforms.ToTensor()(actor_img))\n",
    "                    occluded_image2_pil = transform(occluded_image2)\n",
    "                    Image2copy.paste(occluded_image2_pil, (int(box[0]), int(box[1]), int(box[0] + box[2]), int(box[1] + box[3])))\n",
    "            upload_cv2_image_to_gcs(\n",
    "                     cv2.cvtColor(np.asarray(Image1copy), cv2.COLOR_BGR2RGB),\n",
    "                     f\"{output_path}/{save_subfolder}/{image_name1}\",\n",
    "                    storage_client=storage_client,\n",
    "                )\n",
    "            upload_cv2_image_to_gcs(\n",
    "                    cv2.cvtColor(np.asarray(Image2copy), cv2.COLOR_BGR2RGB),\n",
    "                    f\"{output_path}/{save_subfolder}/{image_name2}\",\n",
    "                    storage_client=storage_client,\n",
    "                )\n",
    "\n",
    "\n",
    "    data['annotations'] = annotations_new\n",
    "    gcs_label_path = f\"{output_path}/{save_subfolder}/annotations.json\"\n",
    "    dump_to_gcs(\n",
    "            gcs_label_path,\n",
    "            json.dumps(data),\n",
    "            content_type=\"application/json\",\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23876cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-bloomingdale-0002', '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-bloomingdale-0005', '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-pooler-0004', '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_uscold-laredo-dock01', '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-pooler-0008', '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-pooler-0007', '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-bloomingdale-0007', '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-bloomingdale-0006', '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-bloomingdale-0004', '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-pooler-0005', '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-pooler-0006', '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-pooler-0009']\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/'\n",
    "data_dirs = [x[0] for x in os.walk(root_dir)][1:]\n",
    "print(data_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45818ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-pooler-0007\n",
      "/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-bloomingdale-0007\n",
      "/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-bloomingdale-0006\n",
      "/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-bloomingdale-0004\n",
      "/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-pooler-0005\n",
      "/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-pooler-0006\n",
      "/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/_americold-savannah-pooler-0009\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, len(data_dirs)):\n",
    "    print(data_dirs[i])\n",
    "    data = json.load(open(f\"{data_dirs[i]}/annotations.json\"))\n",
    "    dict_sample = {}\n",
    "    for name in data['images']:\n",
    "        sample = [anno for anno in data['annotations'] if anno['image_id'] == name['id']]\n",
    "        dict_sample[name['file_name']] = sample\n",
    "    ids = [imgdata['id'] for imgdata in data['images']]\n",
    "    max_id = max(ids)\n",
    "    get_occlusion(data, dict_sample, max_id, data_dirs[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf425bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
