{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0534b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94183f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDataset(Dataset):    \n",
    "    def __init__(self, data_dir = None):\n",
    "        self._data_dir = data_dir\n",
    "        data_dirs = [x[0] for x in os.walk(self._data_dir)][1:]\n",
    "        annotations = []\n",
    "        for i in range(0,len(data_dirs)):\n",
    "            annotation = json.load(open(f\"{data_dirs[i]}/annotations.json\"))['annotations']\n",
    "            annotations.extend(annotation)\n",
    "        self._annotations_person = [ano for ano in annotations if ano['category_id'] == 0]\n",
    "        self._pose_dict = {\"liftingbad\": 0, \n",
    "                           \"liftinggood\": 1, \n",
    "                           \"reachingbad\": 2, \n",
    "                           \"reachinggood\": 3, \n",
    "                           'randomrandom': 4}\n",
    "\n",
    "    def _process_pose(self, pose):\n",
    "        return self._pose_dict[pose]\n",
    "    \n",
    "    def _get_pose_size(self, keypoints, ratio):\n",
    "        hips_center = (keypoints[9,:] + keypoints[10,:]) / 2\n",
    "        shoulders_center = (keypoints[3,:] + keypoints[4,:]) / 2\n",
    "        torso_size = np.linalg.norm((shoulders_center - hips_center))\n",
    "        distance = np.linalg.norm((keypoints - hips_center), axis = 1)\n",
    "        max_d = np.max(distance)\n",
    "        pose_size = max(torso_size * ratio, max_d)\n",
    "        return pose_size\n",
    "    def _normalize_pose(self,keypoints):\n",
    "        data_p = np.expand_dims(np.array(keypoints), axis=1).reshape(-1,3)[:,0:2]\n",
    "        data_p = np.delete(data_p,[3,4], axis = 0)\n",
    "        hip_center = (data_p[9,:] + data_p[10,:]) / 2\n",
    "        data_p = data_p - hip_center\n",
    "        pose_size = self._get_pose_size(data_p, 2)\n",
    "        data_p = data_p / pose_size\n",
    "        return data_p.flatten()\n",
    "    def __getitem__(self, idx):\n",
    "        actor = self._annotations_person[idx]\n",
    "        x = torch.tensor(self._normalize_pose(actor['keypoints']))\n",
    "        pose = actor['pose_category'] + actor['pose_subcategory']\n",
    "        y = self._process_pose(pose)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self._annotations_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeab75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_data = PoseDataset(data_dir = '/home/reza_voxelsafety_com/experiments/ergonomic/ergonomic-Infinity/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "840baa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 47915, 'val': 5989, 'test': 5989}\n"
     ]
    }
   ],
   "source": [
    "num_val= int(len(pose_data) * 0.1)\n",
    "num_train = len(pose_data) - 2 * num_val\n",
    "train, val, test = random_split(pose_data, [num_train, num_val, num_val])\n",
    "batch_size = 128\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8),\n",
    "    'val': DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8),\n",
    "}\n",
    "dataset_sizes = {}\n",
    "dataset_sizes['train'] = len(train)\n",
    "dataset_sizes['val'] = len(val)\n",
    "dataset_sizes['test'] = len(test)\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6356988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of datapoints: 59893\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of datapoints: {len(pose_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d710d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = DataLoader(pose_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c750ea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of one batch input data torch.Size([1, 30])\n",
      "size of one batch label data torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(dataloader_test))\n",
    "print(f\"size of one batch input data {x.shape}\")\n",
    "print(f\"size of one batch label data {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6e9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class PoseModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(PoseModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82193339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1871, 0.2299, 0.2041, 0.1853, 0.1936]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "pose_model = PoseModel(30, 5)\n",
    "out = pose_model(x.float())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e1f8c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1747, 0.2007, 0.1961, 0.2019, 0.2265]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.6077, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "print(out)\n",
    "print(y)\n",
    "loss(out, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab28aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import wandb\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, dataset_sizes, optimizer, config):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    wandb.init(project =  \"ergo_ml_training\", entity = \"voxel-wandb\", config = config, tags = [config['tags']])\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=config['step'], gamma=0.1)\n",
    "    config = wandb.config\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())   \n",
    "    best_accuracy = 0\n",
    "    model.to(device)\n",
    "    today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, config.num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs.float())\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            if phase == 'val':\n",
    "                #print(\"val accuracy\", epoch_acc)\n",
    "                wandb.log({\"val loss\":epoch_loss})\n",
    "                wandb.log({\"val_accuracy\":epoch_acc})\n",
    "            if phase == 'train':\n",
    "                #print(\"train accuracy\", epoch_acc)\n",
    "                #print(\"train loss\", epoch_loss)\n",
    "                wandb.log({\"train loss\":epoch_loss})\n",
    "                wandb.log({\"train_accuracy\":epoch_acc})\n",
    "            print(f'{phase} Loss: {epoch_loss} Acc: {epoch_acc}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_accuracy:\n",
    "                best_accuracy = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_accuracy))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), f\"voxel_ergo_ml_{config.tags}_{today_date}.pth\")\n",
    "    wandb.join()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73bfc836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gtnligtd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14612... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47218d4ac637431dbc3b917486932fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.30MB of 0.30MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>█▅▄▄▄▄▄▄▄▄▄▄▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▄▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇█████████████████████</td></tr><tr><td>val loss</td><td>█▇▇▆▆▆▆▆▆▆▆▆▆▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▂▂▃▃▃▃▃▃▃▃▃▆▇▇▇▇▇▇████████████████████</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>1.003</td></tr><tr><td>train_accuracy</td><td>0.90101</td></tr><tr><td>val loss</td><td>0.97461</td></tr><tr><td>val_accuracy</td><td>0.92887</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ancient-republic-5</strong>: <a href=\"https://wandb.ai/voxel-wandb/ergo_ml_training/runs/gtnligtd\" target=\"_blank\">https://wandb.ai/voxel-wandb/ergo_ml_training/runs/gtnligtd</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220505_182330-gtnligtd/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gtnligtd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.16 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2022-05-05 18:26:55.092141: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-05 18:26:55.092190: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/voxel-wandb/ergo_ml_training/runs/1t9ntwdf\" target=\"_blank\">dark-jawa-6</a></strong> to <a href=\"https://wandb.ai/voxel-wandb/ergo_ml_training\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 1.3764709845572465 Acc: 0.5295418971094648\n",
      "val Loss: 1.1992964968495823 Acc: 0.7144765403239273\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 1.2002458540343124 Acc: 0.7116977981842847\n",
      "val Loss: 1.1626792425583035 Acc: 0.7436967774252797\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 1.1780852417128327 Acc: 0.7295836376917458\n",
      "val Loss: 1.1532839266161903 Acc: 0.7503756887627318\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 1.170954509370078 Acc: 0.7353020974642597\n",
      "val Loss: 1.147275271012043 Acc: 0.757722491233929\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 1.1645185865490593 Acc: 0.7415005739330064\n",
      "val Loss: 1.1449737182183424 Acc: 0.7597261646351645\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 1.1604076843402038 Acc: 0.7456746321611187\n",
      "val Loss: 1.1474575130743223 Acc: 0.75722157288362\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 1.1586881587864348 Acc: 0.7465929249713034\n",
      "val Loss: 1.1405852168803423 Acc: 0.7635665386541994\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 1.1548219775924675 Acc: 0.7505791505791506\n",
      "val Loss: 1.138300116640595 Acc: 0.7660711304057439\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 1.1523832210702785 Acc: 0.7528540123134718\n",
      "val Loss: 1.138982375588913 Acc: 0.7654032392719987\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 1.1510671138962463 Acc: 0.7535427319211103\n",
      "val Loss: 1.137751784163816 Acc: 0.7657371848388713\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 1.1489617062401176 Acc: 0.756172388604821\n",
      "val Loss: 1.1387580337379508 Acc: 0.7644014025713809\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 1.1469958533427667 Acc: 0.7577167901492227\n",
      "val Loss: 1.1356047925536672 Acc: 0.7679078310235432\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 1.1466572861824207 Acc: 0.7580507148074717\n",
      "val Loss: 1.1359315341891183 Acc: 0.7674069126732344\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 1.143977983149907 Acc: 0.7605760200354795\n",
      "val Loss: 1.1290096419744131 Acc: 0.7769243613291035\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 1.1446808763527117 Acc: 0.7604507982886362\n",
      "val Loss: 1.129377364216593 Acc: 0.7747537151444315\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 1.1434122666926494 Acc: 0.761076907022853\n",
      "val Loss: 1.1289963386834694 Acc: 0.7745867423609952\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 1.1411926629410698 Acc: 0.7636856934154231\n",
      "val Loss: 1.1279480538517113 Acc: 0.7757555518450493\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 1.1411204457494495 Acc: 0.7635187310862988\n",
      "val Loss: 1.1307032996056812 Acc: 0.77391885122725\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 1.1405770116194198 Acc: 0.763643952833142\n",
      "val Loss: 1.1258297652468934 Acc: 0.7780931708131575\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 1.1407312023158265 Acc: 0.7632474173014714\n",
      "val Loss: 1.1248078766838898 Acc: 0.779428953080648\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 1.1394183331138963 Acc: 0.7646248565167485\n",
      "val Loss: 1.1252353970645006 Acc: 0.7785940891634664\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 1.138328027976501 Acc: 0.7659605551497444\n",
      "val Loss: 1.1191499255539121 Acc: 0.7849390549340458\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 1.1352727950048462 Acc: 0.7676510487321299\n",
      "val Loss: 1.1194494882277244 Acc: 0.7807647353481383\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 1.0960632301678082 Acc: 0.8111447354690599\n",
      "val Loss: 1.033880388364507 Acc: 0.8729337118049758\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 1.059031469735101 Acc: 0.8491912762183033\n",
      "val Loss: 1.0546456511499567 Acc: 0.8515611955251294\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 1.0477301708706284 Acc: 0.8595638109151623\n",
      "val Loss: 1.017602911077491 Acc: 0.8872933711804977\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 1.0423621075220275 Acc: 0.8642596264217887\n",
      "val Loss: 1.0056424420889878 Acc: 0.9001502755050927\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 1.0338964128971946 Acc: 0.8721277261817802\n",
      "val Loss: 1.0036865915872355 Acc: 0.9001502755050927\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 1.0327932394037265 Acc: 0.8731921110299489\n",
      "val Loss: 1.0002107022699958 Acc: 0.9036567039572551\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 1.026530931491786 Acc: 0.879474068663258\n",
      "val Loss: 0.9913434182130588 Acc: 0.9141759893137419\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 1.0252015470749374 Acc: 0.8797871230303663\n",
      "val Loss: 1.0026160935404942 Acc: 0.901986976122892\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 1.025663416863085 Acc: 0.8797245121569447\n",
      "val Loss: 0.9954402465258203 Acc: 0.9079979963265988\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 1.0221721975661315 Acc: 0.8830428884482939\n",
      "val Loss: 0.9948812648119045 Acc: 0.9091668058106529\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 1.0229800478297237 Acc: 0.8820411144735469\n",
      "val Loss: 0.9951920074121963 Acc: 0.9078310235431626\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 1.0222056217399107 Acc: 0.8826254826254827\n",
      "val Loss: 0.9912005880476538 Acc: 0.9135080981799968\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 1.0178600719138524 Acc: 0.8865073567776272\n",
      "val Loss: 0.9922237644053835 Acc: 0.9115044247787611\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 1.0229152171929992 Acc: 0.8814567463216112\n",
      "val Loss: 0.9926696286155298 Acc: 0.9138420437468693\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 1.018500023032235 Acc: 0.8863403944485027\n",
      "val Loss: 0.9822845374289847 Acc: 0.9221906829186843\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 1.0156998069826477 Acc: 0.8890535322967756\n",
      "val Loss: 0.9852866627254795 Acc: 0.918684254466522\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 1.0163789801068661 Acc: 0.8880726286131692\n",
      "val Loss: 0.9849532419561563 Acc: 0.9193521456002672\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 1.0171261527537554 Acc: 0.8871543358029845\n",
      "val Loss: 0.9894995048161958 Acc: 0.9150108532309235\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 1.016352871849357 Acc: 0.8878013148283419\n",
      "val Loss: 0.9931998721425452 Acc: 0.9111704792118885\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 1.0177171593305525 Acc: 0.8864864864864865\n",
      "val Loss: 0.9805601446710976 Acc: 0.9238604107530474\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 1.014541108889631 Acc: 0.8899718251069603\n",
      "val Loss: 0.9887676932792231 Acc: 0.9146769076640509\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 1.0110560479498003 Acc: 0.8931858499426067\n",
      "val Loss: 0.98244585285682 Acc: 0.9216897645683755\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 1.0122264823671747 Acc: 0.8921005948032975\n",
      "val Loss: 0.984177791508993 Acc: 0.9201870095174487\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 1.0095357332898371 Acc: 0.894980694980695\n",
      "val Loss: 0.9820032956246284 Acc: 0.9223576557021206\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 1.0069226944159209 Acc: 0.8976729625378275\n",
      "val Loss: 0.9787606140730835 Acc: 0.9248622474536651\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 1.007086807425331 Acc: 0.897506000208703\n",
      "val Loss: 0.984705661738092 Acc: 0.9201870095174487\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 1.0093942645122795 Acc: 0.895126787018679\n",
      "val Loss: 0.9777943485765271 Acc: 0.9271998664217733\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 1.0022887971609455 Acc: 0.9027861838672651\n",
      "val Loss: 0.9784991179370068 Acc: 0.9258640841542829\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 1.0069403536543895 Acc: 0.8976103516644057\n",
      "val Loss: 0.9748569447979503 Acc: 0.9285356486892637\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 1.0026707246658586 Acc: 0.9017844098925181\n",
      "val Loss: 0.979958078482649 Acc: 0.9246952746702288\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 1.0080411174445862 Acc: 0.8963790044871126\n",
      "val Loss: 0.9801301360229834 Acc: 0.9233594924027384\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 1.005068969928085 Acc: 0.8991756234999478\n",
      "val Loss: 0.981208809908174 Acc: 0.9231925196193022\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 1.0037373244656684 Acc: 0.9003443598038193\n",
      "val Loss: 0.9800257079090533 Acc: 0.9238604107530474\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 1.002711504642443 Acc: 0.9014922258165502\n",
      "val Loss: 0.9716788271887112 Acc: 0.9325429954917349\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 1.003619573155612 Acc: 0.9008661170823333\n",
      "val Loss: 0.9764115562879256 Acc: 0.9280347303389548\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 1.0020657802864614 Acc: 0.9021392048419076\n",
      "val Loss: 0.9738579055645527 Acc: 0.9312072132242445\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 1.0040742283783985 Acc: 0.9002608786392571\n",
      "val Loss: 0.9762297968053125 Acc: 0.9278677575555185\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.9991989268908779 Acc: 0.9048732129813212\n",
      "val Loss: 0.9726451556040461 Acc: 0.9322090499248623\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 1.0019980186822954 Acc: 0.9024939997912972\n",
      "val Loss: 0.9763379622867219 Acc: 0.9273668392052097\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 1.0041612871850345 Acc: 0.9002817489303976\n",
      "val Loss: 0.9735600276848453 Acc: 0.9298714309567542\n",
      "Epoch 63/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.002199240496123 Acc: 0.9024939997912972\n",
      "val Loss: 0.9693024490388771 Acc: 0.9352145600267158\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.9990077563694545 Acc: 0.9049775644370239\n",
      "val Loss: 0.9740540104988004 Acc: 0.9298714309567542\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.9986594621764212 Acc: 0.9057706355003653\n",
      "val Loss: 0.9722453310306929 Acc: 0.9327099682751713\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 1.0013810530642335 Acc: 0.9036209955128874\n",
      "val Loss: 0.9841592531905665 Acc: 0.9200200367340124\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.999327457551977 Acc: 0.9048732129813212\n",
      "val Loss: 0.976804487393523 Acc: 0.9275338119886459\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.9986969234686387 Acc: 0.9054158405509758\n",
      "val Loss: 0.9779517444889102 Acc: 0.9263650025045919\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.9976940328164525 Acc: 0.9067932797662528\n",
      "val Loss: 0.9708963658957355 Acc: 0.9335448321923527\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.9877422195802983 Acc: 0.9168527600960034\n",
      "val Loss: 0.9663895168334699 Acc: 0.9385540156954417\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.9832750157562063 Acc: 0.9214233538557863\n",
      "val Loss: 0.9663969527642025 Acc: 0.938720988478878\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.9817950255348602 Acc: 0.922821663362204\n",
      "val Loss: 0.9662366179815615 Acc: 0.9373852062113877\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.9795516942645318 Acc: 0.9246373786914328\n",
      "val Loss: 0.96528677726194 Acc: 0.9393888796126232\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.9783583338718679 Acc: 0.9261191693624127\n",
      "val Loss: 0.9643624830393267 Acc: 0.940390716313241\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.978219096409084 Acc: 0.9265991860586456\n",
      "val Loss: 0.9640193397761148 Acc: 0.9397228251794958\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.9780106134595297 Acc: 0.9264530940206617\n",
      "val Loss: 0.9625001585051619 Acc: 0.9410586074469862\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.9771405636835083 Acc: 0.927767922362517\n",
      "val Loss: 0.9634908078349399 Acc: 0.940390716313241\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.9765145990080516 Acc: 0.9280392361473443\n",
      "val Loss: 0.9621204877560555 Acc: 0.9418934713641677\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.976047712984307 Acc: 0.92862360429928\n",
      "val Loss: 0.9628112210736168 Acc: 0.9412255802304225\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.9758472673432459 Acc: 0.9287488260461234\n",
      "val Loss: 0.9611779501828671 Acc: 0.9423943897144766\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.9757576978195439 Acc: 0.9289157883752479\n",
      "val Loss: 0.9622635240944145 Acc: 0.9412255802304225\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.9746218353351297 Acc: 0.9299801732234165\n",
      "val Loss: 0.9611056069934442 Acc: 0.9432292536316581\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.975569697068527 Acc: 0.928811436919545\n",
      "val Loss: 0.9608536204306067 Acc: 0.9438971447654033\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.9742210845664636 Acc: 0.9304601899196494\n",
      "val Loss: 0.9622393680805156 Acc: 0.9422274169310403\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.9741558701648949 Acc: 0.9305019305019305\n",
      "val Loss: 0.9593859527998405 Acc: 0.9448989814660211\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.9727390133636699 Acc: 0.932192424084316\n",
      "val Loss: 0.9595302217471259 Acc: 0.9447320086825848\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.9748345874264708 Acc: 0.9297506000208704\n",
      "val Loss: 0.960364795145158 Acc: 0.9438971447654033\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.9752916760028161 Acc: 0.9293958050714808\n",
      "val Loss: 0.9604807741197328 Acc: 0.9438971447654033\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.9746704820624774 Acc: 0.9298966920588543\n",
      "val Loss: 0.9601014414918823 Acc: 0.9443980631157122\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.9745213514626754 Acc: 0.9299384326411354\n",
      "val Loss: 0.9606524058685296 Acc: 0.9435631991985307\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.9730455174459043 Acc: 0.9313784827298341\n",
      "val Loss: 0.9617666008874916 Acc: 0.9418934713641677\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.9737758449643312 Acc: 0.9306897631221956\n",
      "val Loss: 0.9590301885093706 Acc: 0.9452329270328937\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.9738831213803626 Acc: 0.9308567254513201\n",
      "val Loss: 0.9600619348585556 Acc: 0.9443980631157122\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.9733544758873122 Acc: 0.9313576124386935\n",
      "val Loss: 0.9604983569624668 Acc: 0.9432292536316581\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.9736401766013044 Acc: 0.9306897631221956\n",
      "val Loss: 0.9600438370711021 Acc: 0.9437301719819671\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.9723311359942314 Acc: 0.9323802567045811\n",
      "val Loss: 0.9595691070450748 Acc: 0.9455668725997664\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.9720342036242974 Acc: 0.932860273400814\n",
      "val Loss: 0.9602666636961572 Acc: 0.9438971447654033\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.9731887096255555 Acc: 0.931148909527288\n",
      "val Loss: 0.9592806788652484 Acc: 0.945900818166639\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.9727441643593793 Acc: 0.932004591464051\n",
      "val Loss: 0.9587913495727166 Acc: 0.94539989981633\n",
      "Training complete in 5m 18s\n",
      "Best val Acc: 0.945901\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30900... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86450d55eb645b8b17cc5b38118cada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.31MB of 0.31MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>█▅▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▄▅▅▅▅▅▅▅▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██████████████</td></tr><tr><td>val loss</td><td>█▇▆▆▆▆▆▆▆▆▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▂▃▃▃▃▃▃▃▆▇▇▇▇▇▇▇▇▇▇▇▇███▇▇████████████</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>0.97274</td></tr><tr><td>train_accuracy</td><td>0.932</td></tr><tr><td>val loss</td><td>0.95879</td></tr><tr><td>val_accuracy</td><td>0.9454</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dark-jawa-6</strong>: <a href=\"https://wandb.ai/voxel-wandb/ergo_ml_training/runs/1t9ntwdf\" target=\"_blank\">https://wandb.ai/voxel-wandb/ergo_ml_training/runs/1t9ntwdf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220505_182648-1t9ntwdf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PoseModel(\n",
       "  (layer1): Linear(in_features=30, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer3): Linear(in_features=64, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "        'num_epochs': 100,\n",
    "        'tags': \"ergoMLLinear\",\n",
    "        'step': 70,\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.SGD(pose_model.parameters(), lr=0.1, momentum=0.9)\n",
    "train_model(pose_model, dataloaders, dataset_sizes, optimizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "349db8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test 0.9268659208549007\n"
     ]
    }
   ],
   "source": [
    "model = PoseModel(30, 5)\n",
    "model.load_state_dict(\n",
    "    torch.load('/home/reza_voxelsafety_com/voxel/experimental/reza/Ergonomic/voxel_ergo_ml_ergoMLLinear_2022-05-05.pth')\n",
    ")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataloaders['test'] = DataLoader(val, batch_size=1, shuffle=False, num_workers=8)\n",
    "running_corrects = 0\n",
    "model.to(device)\n",
    "for inputs, labels in dataloaders['test']:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(inputs.float())\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "accuracy_test = running_corrects.double() / dataset_sizes['test']\n",
    "print(f\"Accuracy test {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804187c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
